"""
TextGrad ì—†ì´ ìˆ˜ë™ í”„ë¡¬í”„íŠ¸ ìµœì í™”
GPT-4oë¥¼ ì´ìš©í•œ ë°˜ë³µì  í”„ë¡¬í”„íŠ¸ ê°œì„ 
"""

import os
import json
import random
from typing import List, Dict, Tuple
from gpt4o_time_reader import GPT4oTimeReader
from evaluation_system import SeparateEvaluationSystem
import openai

class ManualPromptOptimizer:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.client = openai.OpenAI(api_key=api_key)
        self.time_reader = GPT4oTimeReader(api_key)
        self.evaluator = SeparateEvaluationSystem()
        
        # ì´ˆê¸° í”„ë¡¬í”„íŠ¸ë“¤
        self.initial_prompts = [
            """ì´ ì•„ë‚ ë¡œê·¸ ì‹œê³„ ì´ë¯¸ì§€ë¥¼ ë³´ê³  ì •í™•í•œ ì‹œê°„ì„ ì½ì–´ì£¼ì„¸ìš”.

ì‘ë‹µ í˜•ì‹:
{
    "hour": ì‹œê°„(0-23),
    "minute": ë¶„(0-59),
    "confidence": í™•ì‹ ë„(0.0-1.0)
}

ì£¼ì˜ì‚¬í•­:
- ì‹œì¹¨(ì§§ê³  êµµì€ ë°”ëŠ˜)ê³¼ ë¶„ì¹¨(ê¸¸ê³  ì–‡ì€ ë°”ëŠ˜)ì„ ì •í™•íˆ êµ¬ë¶„í•˜ì„¸ìš”
- ì‹œì¹¨ì˜ ìœ„ì¹˜ë¡œ ì‹œê°„ì„, ë¶„ì¹¨ì˜ ìœ„ì¹˜ë¡œ ë¶„ì„ ì½ìœ¼ì„¸ìš”
- ì‹œê°„ì€ 24ì‹œê°„ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
- JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”""",

            """ì•„ë‚ ë¡œê·¸ ì‹œê³„ë¥¼ ì •í™•íˆ ì½ì–´ì£¼ì„¸ìš”.

ë¶„ì„ ë°©ë²•:
1. ì§§ê³  êµµì€ ë°”ëŠ˜(ì‹œì¹¨): ì‹œê°„ì„ ë‚˜íƒ€ëƒ„
2. ê¸¸ê³  ì–‡ì€ ë°”ëŠ˜(ë¶„ì¹¨): ë¶„ì„ ë‚˜íƒ€ëƒ„
3. ë¶„ì¹¨ì´ ê°€ë¦¬í‚¤ëŠ” ìˆ«ì Ã— 5 = ë¶„
4. ì‹œì¹¨ì€ ì •ì‹œì™€ ë‹¤ìŒ ì‹œ ì‚¬ì´ì— ìœ„ì¹˜

ì‘ë‹µ í˜•ì‹:
{
    "hour": ì‹œê°„(0-23),
    "minute": ë¶„(0-59),
    "confidence": í™•ì‹ ë„(0.0-1.0)
}

JSONìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.""",

            """Clock Reading Instructions:

Step 1: Identify the hands
- Short, thick hand = HOUR hand
- Long, thin hand = MINUTE hand

Step 2: Read the minute hand first
- Find which number the long hand points to
- Multiply by 5 to get minutes (12=60=0, 1=5, 2=10, 3=15, etc.)

Step 3: Read the hour hand
- The short hand shows the hour
- If it's between two numbers, use the smaller number
- Convert to 24-hour format if needed

Response format:
{
    "hour": hour(0-23),
    "minute": minute(0-59),
    "confidence": confidence(0.0-1.0)
}

Respond only in JSON format."""
        ]
    
    def evaluate_prompt(self, prompt: str, test_data: List[Dict], max_samples: int = 15) -> Tuple[float, Dict]:
        """í”„ë¡¬í”„íŠ¸ í‰ê°€"""
        # ëœë¤ ìƒ˜í”Œ ì„ íƒ
        test_samples = random.sample(test_data, min(max_samples, len(test_data)))
        image_paths = [os.path.join("dataset", sample['filename']) for sample in test_samples]
        
        print(f"Testing prompt with {len(test_samples)} samples...")
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        predictions = self.time_reader.batch_read_times(image_paths, prompt)
        
        # í‰ê°€
        evaluation = self.evaluator.comprehensive_evaluation(predictions, test_samples)
        
        return evaluation['combined_metrics']['exact_match_accuracy'], evaluation
    
    def generate_improved_prompt(self, current_prompt: str, evaluation_results: Dict, failed_examples: List[Dict]) -> str:
        """GPT-4oë¥¼ ì´ìš©í•œ í”„ë¡¬í”„íŠ¸ ê°œì„ """
        
        # ì‹¤íŒ¨ ì‚¬ë¡€ ë¶„ì„
        error_analysis = f"""
í˜„ì¬ ì„±ëŠ¥:
- ì‹œê°„ ì •í™•ë„: {evaluation_results['hour_metrics']['accuracy']:.1%}
- ë¶„ ì •í™•ë„: {evaluation_results['minute_metrics']['accuracy']:.1%}
- ì „ì²´ ë§¤ì¹­: {evaluation_results['combined_metrics']['exact_match_accuracy']:.1%}
- í‰ê·  ì‹œê°„ ì˜¤ì°¨: {evaluation_results['hour_metrics']['mean_absolute_error']:.1f}ì‹œê°„
- í‰ê·  ë¶„ ì˜¤ì°¨: {evaluation_results['minute_metrics']['mean_absolute_error']:.1f}ë¶„

ì‹¤íŒ¨ ì‚¬ë¡€ë“¤:
"""
        
        for i, example in enumerate(failed_examples[:5]):  # ìµœëŒ€ 5ê°œ ì‚¬ë¡€
            error_analysis += f"- ì‹¤ì œ: {example['true_hour']:02d}:{example['true_minute']:02d}, ì˜ˆì¸¡: {example['pred_hour']:02d}:{example['pred_minute']:02d}\n"
        
        improvement_prompt = f"""ë‹¹ì‹ ì€ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë‚ ë¡œê·¸ ì‹œê³„ ì½ê¸° í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•´ì•¼ í•©ë‹ˆë‹¤.

í˜„ì¬ í”„ë¡¬í”„íŠ¸:
{current_prompt}

{error_analysis}

ë¬¸ì œì  ë¶„ì„:
1. ì‹œì¹¨ê³¼ ë¶„ì¹¨ êµ¬ë¶„ì´ ì–´ë ¤ì›€
2. ë¶„ ê³„ì‚° ì˜¤ë¥˜ (ë¶„ì¹¨ ìœ„ì¹˜ Ã— 5)
3. ì‹œê°„ ê³„ì‚° ì˜¤ë¥˜ (ì‹œì¹¨ì´ ë‘ ìˆ«ì ì‚¬ì´ì— ìˆì„ ë•Œ)
4. 24ì‹œê°„ í˜•ì‹ ë³€í™˜ ë¬¸ì œ

ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•´ì•¼ í•©ë‹ˆë‹¤:
1. ì‹œì¹¨ê³¼ ë¶„ì¹¨ì„ ëª…í™•íˆ êµ¬ë¶„í•˜ëŠ” ë°©ë²• ì œì‹œ
2. ë‹¨ê³„ë³„ ì½ê¸° ë°©ë²• ëª…ì‹œ
3. ì¼ë°˜ì ì¸ ì‹¤ìˆ˜ ë°©ì§€ ë°©ë²• í¬í•¨
4. JSON ì‘ë‹µ í˜•ì‹ ìœ ì§€
5. ë” êµ¬ì²´ì ì´ê³  ì •í™•í•œ ì§€ì¹¨ ì œê³µ

ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”:"""

        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": improvement_prompt}],
            max_tokens=800,
            temperature=0.7
        )
        
        return response.choices[0].message.content.strip()
    
    def collect_failed_examples(self, predictions: List[Dict], ground_truth: List[Dict]) -> List[Dict]:
        """ì‹¤íŒ¨ ì‚¬ë¡€ ìˆ˜ì§‘"""
        failed_examples = []
        
        for pred, truth in zip(predictions, ground_truth):
            pred_hour = pred.get('hour', -1)
            pred_minute = pred.get('minute', -1)
            true_hour = truth['hour']
            true_minute = truth['minute']
            
            if pred_hour != true_hour or pred_minute != true_minute:
                failed_examples.append({
                    'filename': truth['filename'],
                    'true_hour': true_hour,
                    'true_minute': true_minute,
                    'pred_hour': pred_hour,
                    'pred_minute': pred_minute,
                    'confidence': pred.get('confidence', 0)
                })
        
        return failed_examples
    
    def optimize_prompt(self, dataset: List[Dict], num_iterations: int = 3) -> str:
        """í”„ë¡¬í”„íŠ¸ ìµœì í™” ë©”ì¸ ë¡œì§"""
        print("=" * 60)
        print("ğŸš€ ì•„ë‚ ë¡œê·¸ ì‹œê³„ í”„ë¡¬í”„íŠ¸ ìµœì í™” ì‹œì‘!")
        print("=" * 60)
        
        # ë°ì´í„° ë¶„í•  (70% í›ˆë ¨, 30% ê²€ì¦)
        random.shuffle(dataset)
        split_idx = int(len(dataset) * 0.7)
        train_data = dataset[:split_idx]
        val_data = dataset[split_idx:]
        
        print(f"í›ˆë ¨ ë°ì´í„°: {len(train_data)}ê°œ, ê²€ì¦ ë°ì´í„°: {len(val_data)}ê°œ")
        
        best_prompt = None
        best_score = 0.0
        optimization_history = []
        
        # ì´ˆê¸° í”„ë¡¬í”„íŠ¸ë“¤ í‰ê°€
        print(f"\nğŸ“Š ì´ˆê¸° í”„ë¡¬í”„íŠ¸ í‰ê°€...")
        for i, prompt in enumerate(self.initial_prompts):
            print(f"\n--- ì´ˆê¸° í”„ë¡¬í”„íŠ¸ {i+1} í…ŒìŠ¤íŠ¸ ---")
            print(f"í”„ë¡¬í”„íŠ¸:\n{prompt}\n")
            score, evaluation = self.evaluate_prompt(prompt, val_data, 10)
            
            print(f"ì„±ëŠ¥: {score:.1%} (ì‹œê°„: {evaluation['hour_metrics']['accuracy']:.1%}, ë¶„: {evaluation['minute_metrics']['accuracy']:.1%})")
            
            if score > best_score:
                best_score = score
                best_prompt = prompt
                print("ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥!")
        
        print(f"\nğŸ† ìµœê³  ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥: {best_score:.1%}")
        
        # ë°˜ë³µì  ê°œì„ 
        current_prompt = best_prompt
        
        for iteration in range(num_iterations):
            print(f"\n{'='*50}")
            print(f"ğŸ”„ ìµœì í™” ë°˜ë³µ {iteration + 1}/{num_iterations}")
            print(f"{'='*50}")
            
            # í˜„ì¬ í”„ë¡¬í”„íŠ¸ë¡œ í›ˆë ¨ ë°ì´í„° í‰ê°€
            train_score, train_evaluation = self.evaluate_prompt(current_prompt, train_data, 20)
            
            # ì‹¤íŒ¨ ì‚¬ë¡€ ìˆ˜ì§‘
            train_samples = random.sample(train_data, 20)
            train_image_paths = [os.path.join("dataset", s['filename']) for s in train_samples]
            train_predictions = self.time_reader.batch_read_times(train_image_paths, current_prompt)
            failed_examples = self.collect_failed_examples(train_predictions, train_samples)
            
            print(f"í›ˆë ¨ ì„±ëŠ¥: {train_score:.1%}")
            print(f"ì‹¤íŒ¨ ì‚¬ë¡€: {len(failed_examples)}ê°œ")
            
            # í”„ë¡¬í”„íŠ¸ ê°œì„ 
            print("ğŸ› ï¸  í”„ë¡¬í”„íŠ¸ ê°œì„  ì¤‘...")
            try:
                improved_prompt = self.generate_improved_prompt(current_prompt, train_evaluation, failed_examples)
                
                print(f"\nğŸ“ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸:\n{'-'*50}\n{improved_prompt}\n{'-'*50}")
                
                # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ê²€ì¦
                print("âœ… ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ê²€ì¦ ì¤‘...")
                new_score, new_evaluation = self.evaluate_prompt(improved_prompt, val_data, 15)
                
                print(f"ê°œì„  í›„ ì„±ëŠ¥: {new_score:.1%} (ë³€í™”: {new_score - best_score:+.1%})")
                
                if new_score > best_score:
                    best_score = new_score
                    best_prompt = improved_prompt
                    current_prompt = improved_prompt
                    print("ğŸ‰ ì„±ëŠ¥ ê°œì„  ì„±ê³µ!")
                else:
                    print("ğŸ“ˆ ì„±ëŠ¥ ê°œì„  ì—†ìŒ, ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ ìœ ì§€")
                
                optimization_history.append({
                    'iteration': iteration + 1,
                    'train_score': train_score,
                    'val_score': new_score,
                    'improvement': new_score - best_score,
                    'prompt': improved_prompt
                })
                
            except Exception as e:
                print(f"âŒ í”„ë¡¬í”„íŠ¸ ê°œì„  ì‹¤íŒ¨: {e}")
                break
        
        # ìµœì¢… ê²°ê³¼
        print(f"\n{'='*60}")
        print("ğŸ¯ ìµœì í™” ì™„ë£Œ!")
        print(f"{'='*60}")
        print(f"ìµœì¢… ì„±ëŠ¥: {best_score:.1%}")
        
        # ìµœì¢… í‰ê°€
        final_score, final_evaluation = self.evaluate_prompt(best_prompt, val_data, len(val_data))
        
        print(f"ìµœì¢… ê²€ì¦ ì„±ëŠ¥:")
        print(f"  ì „ì²´ ë§¤ì¹­: {final_evaluation['combined_metrics']['exact_match_accuracy']:.1%}")
        print(f"  ì‹œê°„ ì •í™•ë„: {final_evaluation['hour_metrics']['accuracy']:.1%}")
        print(f"  ë¶„ ì •í™•ë„: {final_evaluation['minute_metrics']['accuracy']:.1%}")
        
        # ê²°ê³¼ ì €ì¥
        with open('optimization_history.json', 'w', encoding='utf-8') as f:
            json.dump(optimization_history, f, ensure_ascii=False, indent=2)
        
        with open('optimized_prompt.txt', 'w', encoding='utf-8') as f:
            f.write(best_prompt)
        
        print(f"\nğŸ“ ê²°ê³¼ ì €ì¥:")
        print(f"  - optimization_history.json")
        print(f"  - optimized_prompt.txt")
        
        return best_prompt

def main():
    api_key = os.getenv('OPENAI_API_KEY')
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    with open('dataset/metadata.json', 'r', encoding='utf-8') as f:
        dataset = json.load(f)
    
    # ìµœì í™” ì‹¤í–‰
    optimizer = ManualPromptOptimizer(api_key)
    optimized_prompt = optimizer.optimize_prompt(dataset, num_iterations=3)
    
    print(f"\nğŸ‰ ìµœì í™” ì™„ë£Œ!")
    print(f"ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸:")
    print("-" * 40)
    print(optimized_prompt)

if __name__ == "__main__":
    main()