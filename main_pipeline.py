"""
ì „ì²´ ì‹œê³„ ì‹œê°„ ì½ê¸° íŒŒì´í”„ë¼ì¸
ë°ì´í„°ì…‹ ìƒì„± â†’ GPT-4o ì‹œê°„ ì½ê¸° â†’ TextGrad ìµœì í™” â†’ í‰ê°€
"""

import os
import json
import argparse
from datetime import datetime
from dataset_generator import ClockDatasetGenerator
from gpt4o_time_reader import GPT4oTimeReader
from textgrad_optimizer import TimeReadingOptimizer
from evaluation_system import SeparateEvaluationSystem

class TimeReadingPipeline:
    def __init__(self, openai_api_key: str = None):
        self.openai_api_key = openai_api_key or os.getenv('OPENAI_API_KEY')
        self.results = {}
    
    def step1_generate_dataset(self, num_samples: int = 500):
        """Step 1: ê°€ìƒ ì‹œê³„ ë°ì´í„°ì…‹ ìƒì„±"""
        print("=" * 50)
        print("Step 1: Generating Clock Dataset")
        print("=" * 50)
        
        generator = ClockDatasetGenerator()
        dataset = generator.generate_dataset(num_samples)
        
        print(f"âœ“ Generated {len(dataset)} clock images")
        self.results['dataset_size'] = len(dataset)
        return dataset
    
    def step2_baseline_evaluation(self, dataset: list, sample_size: int = 50):
        """Step 2: ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¡œ ì„±ëŠ¥ í‰ê°€"""
        print("\n" + "=" * 50)
        print("Step 2: Baseline Evaluation")
        print("=" * 50)
        
        reader = GPT4oTimeReader(self.openai_api_key)
        evaluator = SeparateEvaluationSystem()
        
        # í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ì„ íƒ
        test_samples = dataset[:sample_size]
        image_paths = [os.path.join("dataset", sample['filename']) for sample in test_samples]
        
        print(f"Testing baseline with {len(test_samples)} samples...")
        predictions = reader.batch_read_times(image_paths)
        
        # í‰ê°€
        baseline_eval = evaluator.comprehensive_evaluation(predictions, test_samples)
        
        print(f"âœ“ Baseline Hour Accuracy: {baseline_eval['hour_metrics']['accuracy']:.2%}")
        print(f"âœ“ Baseline Minute Accuracy: {baseline_eval['minute_metrics']['accuracy']:.2%}")
        print(f"âœ“ Baseline Exact Match: {baseline_eval['combined_metrics']['exact_match_accuracy']:.2%}")
        
        # ë³´ê³ ì„œ ì €ì¥
        evaluator.generate_report(baseline_eval, "baseline_evaluation.json")
        evaluator.plot_results(baseline_eval, "baseline_plots")
        
        self.results['baseline'] = baseline_eval
        return baseline_eval
    
    def step3_optimize_prompt(self, dataset: list):
        """Step 3: TextGradë¡œ í”„ë¡¬í”„íŠ¸ ìµœì í™”"""
        print("\n" + "=" * 50)
        print("Step 3: Prompt Optimization with TextGrad")
        print("=" * 50)
        
        optimizer = TimeReadingOptimizer(self.openai_api_key)
        
        try:
            optimized_prompt, final_score = optimizer.run_optimization()
            
            print(f"âœ“ Prompt optimization completed")
            print(f"âœ“ Optimized prompt score: {final_score:.2%}")
            
            self.results['optimization'] = {
                'final_score': final_score,
                'optimized_prompt': optimized_prompt
            }
            
            return optimized_prompt
            
        except Exception as e:
            print(f"âœ— Optimization failed: {e}")
            print("Using best initial prompt as fallback...")
            
            # ìµœê³  ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì„ íƒ
            best_prompt = optimizer.initial_prompts[0]
            self.results['optimization'] = {
                'final_score': 0.0,
                'optimized_prompt': best_prompt,
                'error': str(e)
            }
            
            return best_prompt
    
    def step4_final_evaluation(self, dataset: list, optimized_prompt: str, sample_size: int = 100):
        """Step 4: ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ë¡œ ìµœì¢… í‰ê°€"""
        print("\n" + "=" * 50)
        print("Step 4: Final Evaluation with Optimized Prompt")
        print("=" * 50)
        
        reader = GPT4oTimeReader(self.openai_api_key)
        evaluator = SeparateEvaluationSystem()
        
        # í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ì„ íƒ (ê¸°ë³¸ í‰ê°€ì™€ ë‹¤ë¥¸ ìƒ˜í”Œ ì‚¬ìš©)
        test_samples = dataset[-sample_size:] if len(dataset) >= sample_size else dataset
        image_paths = [os.path.join("dataset", sample['filename']) for sample in test_samples]
        
        print(f"Testing optimized prompt with {len(test_samples)} samples...")
        predictions = reader.batch_read_times(image_paths, optimized_prompt)
        
        # í‰ê°€
        final_eval = evaluator.comprehensive_evaluation(predictions, test_samples)
        
        print(f"âœ“ Final Hour Accuracy: {final_eval['hour_metrics']['accuracy']:.2%}")
        print(f"âœ“ Final Minute Accuracy: {final_eval['minute_metrics']['accuracy']:.2%}")
        print(f"âœ“ Final Exact Match: {final_eval['combined_metrics']['exact_match_accuracy']:.2%}")
        
        # ë³´ê³ ì„œ ì €ì¥
        evaluator.generate_report(final_eval, "final_evaluation.json")
        evaluator.plot_results(final_eval, "final_plots")
        
        self.results['final'] = final_eval
        return final_eval
    
    def step5_comparison_report(self):
        """Step 5: ìµœì¢… ë¹„êµ ë³´ê³ ì„œ ìƒì„±"""
        print("\n" + "=" * 50)
        print("Step 5: Generating Comparison Report")
        print("=" * 50)
        
        if 'baseline' not in self.results or 'final' not in self.results:
            print("âœ— Cannot generate comparison - missing evaluation results")
            return
        
        baseline = self.results['baseline']
        final = self.results['final']
        
        comparison = {
            'timestamp': datetime.now().isoformat(),
            'dataset_size': self.results['dataset_size'],
            'baseline_results': {
                'hour_accuracy': baseline['hour_metrics']['accuracy'],
                'minute_accuracy': baseline['minute_metrics']['accuracy'],
                'exact_match': baseline['combined_metrics']['exact_match_accuracy']
            },
            'final_results': {
                'hour_accuracy': final['hour_metrics']['accuracy'],
                'minute_accuracy': final['minute_metrics']['accuracy'],
                'exact_match': final['combined_metrics']['exact_match_accuracy']
            },
            'improvements': {
                'hour_accuracy': final['hour_metrics']['accuracy'] - baseline['hour_metrics']['accuracy'],
                'minute_accuracy': final['minute_metrics']['accuracy'] - baseline['minute_metrics']['accuracy'],
                'exact_match': final['combined_metrics']['exact_match_accuracy'] - baseline['combined_metrics']['exact_match_accuracy']
            }
        }
        
        # ë¹„êµ ë³´ê³ ì„œ ì €ì¥
        with open('comparison_report.json', 'w', encoding='utf-8') as f:
            json.dump(comparison, f, ensure_ascii=False, indent=2)
        
        # í…ìŠ¤íŠ¸ ìš”ì•½
        report = f"""
ì‹œê³„ ì‹œê°„ ì½ê¸° í”„ë¡¬í”„íŠ¸ ìµœì í™” ê²°ê³¼ ë¹„êµ
=========================================

ë°ì´í„°ì…‹ í¬ê¸°: {comparison['dataset_size']}ê°œ

ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥:
- ì‹œê°„ ì •í™•ë„: {comparison['baseline_results']['hour_accuracy']:.2%}
- ë¶„ ì •í™•ë„: {comparison['baseline_results']['minute_accuracy']:.2%}
- ì „ì²´ ë§¤ì¹­: {comparison['baseline_results']['exact_match']:.2%}

ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥:
- ì‹œê°„ ì •í™•ë„: {comparison['final_results']['hour_accuracy']:.2%}
- ë¶„ ì •í™•ë„: {comparison['final_results']['minute_accuracy']:.2%}
- ì „ì²´ ë§¤ì¹­: {comparison['final_results']['exact_match']:.2%}

ê°œì„  íš¨ê³¼:
- ì‹œê°„ ì •í™•ë„: {comparison['improvements']['hour_accuracy']:+.2%}
- ë¶„ ì •í™•ë„: {comparison['improvements']['minute_accuracy']:+.2%}
- ì „ì²´ ë§¤ì¹­: {comparison['improvements']['exact_match']:+.2%}

ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸:
{self.results.get('optimization', {}).get('optimized_prompt', 'N/A')}
"""
        
        with open('comparison_report.txt', 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(report)
        print("âœ“ Comparison report saved")
    
    def run_full_pipeline(self, num_samples: int = 500, 
                         baseline_samples: int = 50, 
                         final_samples: int = 100):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("Starting Time Reading Optimization Pipeline")
        print(f"Parameters: {num_samples} samples, baseline: {baseline_samples}, final: {final_samples}")
        
        start_time = datetime.now()
        
        try:
            # Step 1: ë°ì´í„°ì…‹ ìƒì„±
            dataset = self.step1_generate_dataset(num_samples)
            
            # Step 2: ê¸°ë³¸ í‰ê°€
            baseline_eval = self.step2_baseline_evaluation(dataset, baseline_samples)
            
            # Step 3: í”„ë¡¬í”„íŠ¸ ìµœì í™”
            optimized_prompt = self.step3_optimize_prompt(dataset)
            
            # Step 4: ìµœì¢… í‰ê°€
            final_eval = self.step4_final_evaluation(dataset, optimized_prompt, final_samples)
            
            # Step 5: ë¹„êµ ë³´ê³ ì„œ
            self.step5_comparison_report()
            
            end_time = datetime.now()
            duration = end_time - start_time
            
            print("\n" + "=" * 50)
            print("Pipeline Completed Successfully!")
            print("=" * 50)
            print(f"Total duration: {duration}")
            print(f"Generated files:")
            print("- dataset/ (clock images)")
            print("- baseline_evaluation.json/txt")
            print("- optimized_prompt.txt")
            print("- final_evaluation.json/txt")
            print("- comparison_report.json/txt")
            print("- baseline_plots/ and final_plots/")
            
            return True
            
        except Exception as e:
            print(f"\nâœ— Pipeline failed: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    parser = argparse.ArgumentParser(description='Clock Time Reading Optimization Pipeline')
    parser.add_argument('--samples', type=int, default=500, 
                       help='Number of clock images to generate')
    parser.add_argument('--baseline-samples', type=int, default=50,
                       help='Number of samples for baseline evaluation')
    parser.add_argument('--final-samples', type=int, default=100,
                       help='Number of samples for final evaluation')
    parser.add_argument('--api-key', type=str, 
                       help='OpenAI API key (or set OPENAI_API_KEY env var)')
    
    args = parser.parse_args()
    
    # API í‚¤ í™•ì¸
    api_key = args.api_key or os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("Error: OpenAI API key is required.")
        print("Set OPENAI_API_KEY environment variable or use --api-key argument")
        return
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = TimeReadingPipeline(api_key)
    success = pipeline.run_full_pipeline(
        num_samples=args.samples,
        baseline_samples=args.baseline_samples,
        final_samples=args.final_samples
    )
    
    if success:
        print("\nğŸ‰ Pipeline completed successfully!")
    else:
        print("\nâŒ Pipeline failed!")

if __name__ == "__main__":
    main()